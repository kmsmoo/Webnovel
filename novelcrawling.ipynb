{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "from dateutil.parser import parse "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Novel Class\n",
    "* 101 로맨스\n",
    "* 102 SF & 판타지\n",
    "* 103 무협\n",
    "* 104 미스터리\n",
    "* 105 역사&전쟁 (베스트&첼린지)\n",
    "* 106 라이트노벨\n",
    "* 107 팬픽 (첼린지)\n",
    "* 108 퓨전\n",
    "\n",
    "\n",
    "* webnovel 오늘의 웹소설\n",
    "* best 베스트리그\n",
    "* challenge 첼린지리그\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전체 소설 ID 크롤링 (종류, 장르, ID, 작가이름, 총 화수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def __get_novel_ID(genre, level):\n",
    "    \"\"\"\n",
    "    genre = 101~108\n",
    "    level = webnovel, best, challenge\n",
    "    1개의 genre의 완결, 미완결 소설들의 ID와 이름, 총 화수를 수집\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(columns=[\"level\", \"genre\", \"ID\", \"name\", \"episode_total\"])\n",
    "\n",
    "    \n",
    "    if level == \"webnovel\":\n",
    "        number = 2\n",
    "        df = pd.DataFrame(columns=[\"level\", \"genre\", \"ID\", \"name\", \"episode_total\", \"is_fin\"])\n",
    "    else:\n",
    "        number = 1\n",
    "        df = pd.DataFrame(columns=[\"level\", \"genre\", \"ID\", \"name\", \"episode_total\"])\n",
    "        \n",
    "    for fin in [\"\", \"&order=Read&finish=true\"][0:number]:    # 미완결과 완결; 완결은 webnovel 일 때만 존재함\n",
    "        url = \"http://novel.naver.com/{0}/genre.nhn?genre={1}{2}&page=1000\".format(level, genre, fin)\n",
    "        response = requests.get(url)\n",
    "        dom = BeautifulSoup(response.content, \"lxml\")\n",
    "        end_page = dom.select_one(\"div.paging\")    # 최종 페이지 number를 수집\n",
    "        if end_page:\n",
    "            end_page = int(end_page.select_one(\"strong\").contents[0])\n",
    "        else:\n",
    "            end_page = 1\n",
    "\n",
    "        for page in range(1, end_page+1):    # 페이지 수 많큼의 반복문\n",
    "            url = \"http://novel.naver.com/{0}/genre.nhn?genre={1}{2}&page={3}\".format(level, genre, fin, page)\n",
    "            response = requests.get(url)\n",
    "            dom = BeautifulSoup(response.content, \"lxml\")\n",
    "            list_item = dom.select(\"a.list_item\")\n",
    "\n",
    "            for item in list_item:    # 페이지 내 한 개의 소설 정보 수집\n",
    "                novel_ID = item[\"href\"].split(\"=\")[1]\n",
    "                novel_name = item.select_one(\"span.ellipsis\").text\n",
    "                novel_episode_total = item.select_one(\"span.num_total\").text.split(\" \")[1][:-1]\n",
    "                \n",
    "                if level == \"webnovel\":\n",
    "                    if fin == \"\":\n",
    "                        df.loc[len(df)] =  level, genre, novel_ID, novel_name, novel_episode_total, 0\n",
    "                    else:\n",
    "                        df.loc[len(df)] =  level, genre, novel_ID, novel_name, novel_episode_total, 1\n",
    "                else:        \n",
    "                    df.loc[len(df)] =  level, genre, novel_ID, novel_name, novel_episode_total\n",
    "\n",
    "    return df\n",
    "\n",
    "def make_genre_df(level):\n",
    "    \"\"\"\n",
    "    level = webnovel, best, challenge\n",
    "    해당 level의 모든 장르의 각 소설 기본 정보 수집\n",
    "    \"\"\"\n",
    "    if \"data\" not in os.listdir(\".\"):\n",
    "        os.mkdir(\"data\")\n",
    "    if level not in os.listdir(\"data\"):\n",
    "        os.mkdir(os.path.join(\"data\", level))\n",
    "    \n",
    "    genres = [101, 102, 103, 104, 106, 108]\n",
    "    if level == \"best\":    # best의 경우 역사/전쟁이 추가\n",
    "        genres += [105]\n",
    "    if level == \"challenge\":    # challenge의 경우 역사/전쟁에 이어 팬픽이 추가\n",
    "        genres += [105, 107]\n",
    "    \n",
    "    genre_data = map(__get_novel_ID, genres, [level] * len(genres))\n",
    "    genre_df = pd.concat(genre_data).reset_index(drop=True)\n",
    "    genre_df.drop_duplicates(inplace=True)\n",
    "    \n",
    "    like_url = \"http://novel.naver.com/likeCountJson.nhn?contentsIds=\" \\\n",
    "                + (\",\").join(genre_df[\"ID\"])    #like 점수는 다른 곳에서 가져와야 함.\n",
    "    genre_df[\"main_likeit\"] = [\n",
    "        i[\"likeItCount\"]\n",
    "        for i in requests.get(like_url).json()[\"result\"][\"contents\"]\n",
    "    ]\n",
    "    \n",
    "    genre_df[\"episode_total\"] = genre_df[\"episode_total\"].astype(\"int\")\n",
    "    genre_df[\"genre\"] = genre_df[\"genre\"].astype(\"int\")\n",
    "    genre_df.to_csv(os.path.join(\"data\", level, \"genre_df.csv\"))\n",
    "    \n",
    "    return genre_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 소설 score 크롤링 (별점, 관심수, 댓글수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_comment_count(ID, level, get_main=True, number=0):\n",
    "    \"\"\"\n",
    "    get_main\n",
    "    소설의 총 댓글 수만 가져오는 함수\n",
    "    \"\"\"\n",
    "    \n",
    "    if level == \"webnovel\":\n",
    "        level = \"novel01\"\n",
    "    else:\n",
    "        level = \"novel02\"\n",
    "    \n",
    "    headers = {\"Referer\": \"http://novel.naver.com/{level}/list.nhn?\".format(level=level)}\n",
    "    if get_main:    # 소설의 전체 댓글 수\n",
    "        data = {\n",
    "            \"ticket\": level,\n",
    "            \"object_id\": \"novel-{ID}\".format(ID=ID),\n",
    "            \"_ts\": \"1469461095606\",\n",
    "            \"page_size\": \"10\",\n",
    "            \"page_no\": \"1\",\n",
    "            \"sort\": \"newest\"\n",
    "        }\n",
    "    else:    # 소설의 각 에피소드 댓글 수\n",
    "        data = {\n",
    "            \"ticket\": level,\n",
    "            \"object_id\": \"{ID}-{number}\".format(ID=ID, number=number),\n",
    "            \"_ts\": \"1469461095606\",\n",
    "            \"page_size\": \"10\",\n",
    "            \"page_no\": \"1\",\n",
    "            \"sort\": \"newest\"\n",
    "        }\n",
    "        \n",
    "    comment_response = requests.post(\n",
    "        \"http://novel.naver.com/comments/list_comment.nhn\", \n",
    "        headers=headers, \n",
    "        data=data\n",
    "    )\n",
    "    total_count = json.loads(comment_response.text.replace(\"\\\\'\", \"\\'\"))['total_count']\n",
    "    \n",
    "    return total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_novel_data(ID, level):  \n",
    "    url = \"http://novel.naver.com/{level}/list.nhn?novelId={ID}\".format(level=level, ID=ID)\n",
    "    response = requests.get(url)\n",
    "    dom = BeautifulSoup(response.content, \"lxml\")\n",
    "    \n",
    "    main_score = float(dom.select_one(\"p.grade_area\").select_one(\"em\").text) # 메인 별점\n",
    "    concern_count = int(dom.select_one(\"span#concernCount\").text.replace(\",\", \"\")) # 관심도\n",
    "    comments_count = get_comment_count(ID, level)\n",
    "    \n",
    "    return ID, main_score, concern_count, comments_count\n",
    "\n",
    "def make_novel_df(genre_df):\n",
    "    \"\"\"\n",
    "    총 볆점, 댓글수, 관심수를 가져옴\n",
    "    \"\"\"\n",
    "    data = list(map(get_novel_data, genre_df[\"ID\"], genre_df[\"level\"]))\n",
    "    novel_df = pd.DataFrame(data, columns=[\"ID\", \"main_score\", \"concern_count\", \"comments_count\"])\n",
    "    novel_df.drop_duplicates(inplace=True)\n",
    "    novel_df.reset_index(drop=True, inplace=True)\n",
    "    novel_df.to_csv(\"data/{level}/novel_df.csv\".format(level=genre_df[\"level\"][0]))\n",
    "    \n",
    "    return novel_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 소설 크롤링 merge data 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_main_df(level):\n",
    "    \"\"\"\n",
    "    level = webnovel, best, challenge\n",
    "    위의 make_genre_df, make_novel_df의 함수를 이용한 데이터 자동 실행 및 merge\n",
    "    \"\"\"\n",
    "\n",
    "    if \"genre_df.csv\" in os.listdir(os.path.join(\"data\", level)):\n",
    "        try:\n",
    "            genre_df = pd.read_csv(\n",
    "                os.path.join(\"data\", level, \"genre_df.csv\"), \n",
    "                index_col=0\n",
    "            )\n",
    "        except:\n",
    "            genre_df = pd.read_csv(\n",
    "                os.path.join(\"data\", level, \"genre_df.csv\"), \n",
    "                index_col=0, \n",
    "                encoding=\"cp949\"\n",
    "            )\n",
    "    else:\n",
    "        genre_df = make_genre_df(level)\n",
    "\n",
    "    if \"novel_df.csv\" in os.listdir(os.path.join(\"data\", level)):\n",
    "        try:\n",
    "            novel_df = pd.read_csv(\n",
    "                os.path.join(\"data\", level, \"novel_df.csv\"), \n",
    "                index_col=0\n",
    "            )\n",
    "        except:\n",
    "            novel_df = pd.read_csv(\n",
    "                os.path.join(\"data\", level, \"novel_df.csv\"), \n",
    "                index_col=0,\n",
    "                encoding=\"cp949\"\n",
    "            )\n",
    "    else:\n",
    "        novel_df = make_novel_df(genre_df)\n",
    "    \n",
    "    main_df = genre_df.merge(novel_df, on=\"ID\")\n",
    "    main_df.to_csv(os.path.join(\"data\", level, \"main_df.csv\"))\n",
    "    \n",
    "    return main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_df = make_main_df(\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "webnovel_df = make_main_df(\"webnovel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 소설 episode 크롤링 (제목, url_volume, 몇 화, 시간, 하트수, (조회수), 댓글 수, 별점, 별점수, 글)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_text(episode_url):\n",
    "    \"\"\"\n",
    "    episode_url = 소설 1화 url\n",
    "    소설 내용과 하트를 누른 카운터를 반환\n",
    "    \"\"\"\n",
    "    episode_response = requests.get(episode_url)\n",
    "    dom = BeautifulSoup(episode_response.content, \"lxml\")\n",
    "    score_count = int(dom.select_one(\"span#currentStarScoreCount\").text.replace(\",\",\"\")[:-1])\n",
    "    text = dom.select_one(\"div.detail_view_content\").text.replace(\"\\r\\n\", \"\")\n",
    "    \n",
    "    return [score_count, text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_novel_episode(ID, level, episode_total, reset=False):\n",
    "\n",
    "    file_name = \"{ID}-{level}.pickle\".format(ID=ID, level=level)\n",
    "    if (file_name in os.listdir(\"data/episode\")) & (reset == False):\n",
    "        return pd.read_pickle(os.path.join(\"data\", \"episode\", file_name))\n",
    "    \n",
    "    pages = (episode_total // 10) + 2 if episode_total % 10 > 0 else (episode_total // 10) + 1\n",
    "\n",
    "    titles = []\n",
    "    volumes = []\n",
    "    times = []\n",
    "    scores = []\n",
    "    hits = []\n",
    "    \n",
    "    print(ID)\n",
    "    for page in range(1, pages):\n",
    "        \n",
    "        episode_url = \"http://novel.naver.com/{level}/list.nhn?novelId={ID}&page={page}\".format(\n",
    "            level = level,\n",
    "            ID = ID,\n",
    "            page = page\n",
    "        )\n",
    "        \n",
    "        episode_response = requests.get(episode_url)\n",
    "        dom = BeautifulSoup(episode_response.content, \"lxml\")\n",
    "        titles += [\n",
    "            i.text\n",
    "            for i in dom.select_one(\"ul.list_type2.v3.NE=a:lst\").select(\"p.subj.v3\")\n",
    "            if i.text != \"게시 보류중\"\n",
    "        ]\n",
    "        volumes += [\n",
    "            i[\"href\"].split(\"=\")[-1]\n",
    "            for i in dom.select_one(\"ul.list_type2.v3.NE=a:lst\").select(\"a.list_item.NPI=a:list\")\n",
    "            if i\n",
    "        ]\n",
    "        times += [\n",
    "            i.text[:-1] if len(i.text) > 8 else \"2016.07.30\"\n",
    "            for i in dom.select_one(\"ul.list_type2.v3.NE=a:lst\").select(\"span.date\")\n",
    "        ]\n",
    "        scores += [\n",
    "            float(i.text)\n",
    "            for i in dom.select_one(\"ul.list_type2.v3.NE=a:lst\").select(\"span.score_area em\")\n",
    "        ]\n",
    "        if level != \"webnovel\":\n",
    "            hits += [\n",
    "                int(i.text.replace(\",\", \"\"))\n",
    "                for i in dom.select_one(\"ul.list_type2.v3.NE=a:lst\").select(\"span.favorite em\")\n",
    "            ]\n",
    "\n",
    "    like_url = \"http://novel.naver.com/likeCountJson.nhn?contentsIds=\" + str(ID) + \"_\" + (\",\" + str(ID) + \"_\").join(volumes)\n",
    "    likeits = [\n",
    "        i[\"likeItCount\"]\n",
    "        for i in requests.get(like_url).json()[\"result\"][\"contents\"]\n",
    "    ]\n",
    "\n",
    "    comments_count = [\n",
    "        get_comment_count(ID, level, False, i)\n",
    "        for i in volumes\n",
    "    ]\n",
    "\n",
    "    episode_urls = [\n",
    "        \"http://novel.naver.com/{level}/detail.nhn?novelId={ID}&volumeNo={episode}\".format(\n",
    "            level = level, \n",
    "            ID = ID, \n",
    "            episode = i\n",
    "        )\n",
    "        for i in volumes\n",
    "    ]\n",
    "\n",
    "    score_count, text = zip(\n",
    "        *[\n",
    "            get_text(url)\n",
    "            for url in episode_urls\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    episodes = [\n",
    "        i\n",
    "        for i in range(1, len(volumes)+1)\n",
    "    ][::-1]\n",
    "\n",
    "    if level != \"webnovel\":\n",
    "        episode_df = pd.DataFrame(\n",
    "            list(zip(titles, volumes, episodes, times, likeits, hits, comments_count, scores, score_count, text)), \n",
    "            columns=[\"title\", \"volume\", \"episode\", \"time\", \"likeit\", \"hit\", \"comments_count\", \"score\", \"score_count\", \"text\"]\n",
    "        )\n",
    "    else:\n",
    "        episode_df = pd.DataFrame(\n",
    "            list(zip(titles, volumes, episodes, times, likeits, comments_count, scores, score_count, text)), \n",
    "            columns=[\"title\", \"volume\", \"episode\", \"time\", \"likeit\", \"comments_count\", \"score\", \"score_count\", \"text\"]\n",
    "        ) \n",
    "        \n",
    "    episode_df[\"level\"] = level\n",
    "    episode_df[\"ID\"] = ID\n",
    "    \n",
    "    \n",
    "    episode_df.to_pickle(os.path.join(\"data\", \"episode\", file_name))\n",
    "    \n",
    "    return episode_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_episode_df(level, reset=False):\n",
    "    \n",
    "    if \"main_df.csv\" in os.listdir(os.path.join(\"data\", level)):\n",
    "        try:\n",
    "            main_df = pd.read_csv(os.path.join('data', level, \"main_df.csv\"), index_col=0)\n",
    "        except:\n",
    "            main_df = pd.read_csv(os.path.join('data', level, \"main_df.csv\"), index_col=0, encoding=\"cp949\") \n",
    "    else:\n",
    "        main_df = make_main_df(level)\n",
    "    main_df = main_df[main_df[\"episode_total\"] != 0]\n",
    "    \n",
    "    data = list(map(get_novel_episode, main_df[\"ID\"], main_df[\"level\"], main_df[\"episode_total\"], [reset]*len(main_df)))\n",
    "    episode_df = pd.concat(data)\n",
    "    episode_df.reset_index(drop=True, inplace=True)\n",
    "    episode_df.to_csv(os.path.join(\"data\", main_df[\"level\"][0], \"episode_df.csv\"))\n",
    "    \n",
    "    return episode_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "466391\n"
     ]
    }
   ],
   "source": [
    "episode_df = make_episode_df(\"webnovel\", reset=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## novel comment crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_novel_comments(ID, comments_count, get_main, level, reset, number=0):\n",
    "    \n",
    "    file_name = \"{ID}-{number}.pickle\".format(ID=ID, number=number)\n",
    "    \n",
    "    if \"comment\" not in os.listdir(\"data\"):\n",
    "        os.mkdir(os.path.join(\"data\", \"comment\"))\n",
    "    \n",
    "    if (file_name in os.listdir(os.path.join(\"data\", \"comment\"))) & (reset == False):\n",
    "        return pd.read_pickle(os.path.join(\"data\", \"comment\", file_name))\n",
    "    \n",
    "    if level == \"webnovel\":\n",
    "        novel = \"novel01\"\n",
    "    else:\n",
    "        novel = \"novel02\"\n",
    "    \n",
    "    headers = {\"Referer\": \"http://novel.naver.com/{level}/list.nhn?\".format(level=level)}\n",
    "    \n",
    "    comment_list = []\n",
    "    pages = (comments_count // 100) + 1\n",
    "    for page in range(1, pages+1):\n",
    "        \n",
    "        if get_main:\n",
    "            data = {\n",
    "                \"ticket\": novel,\n",
    "                \"object_id\": \"novel-{ID}\".format(ID=ID),\n",
    "                \"_ts\": \"1469461095606\",\n",
    "                \"page_size\": \"100\",\n",
    "                \"page_no\": page,\n",
    "                \"sort\": \"newest\"\n",
    "            }\n",
    "        else:\n",
    "            data = {\n",
    "                \"ticket\": novel,\n",
    "                \"object_id\": \"{ID}-{number}\".format(ID=ID, number=number),\n",
    "                \"_ts\": \"1469461095606\",\n",
    "                \"page_size\": \"100\",\n",
    "                \"page_no\": page,\n",
    "                \"sort\": \"newest\"\n",
    "            }\n",
    "\n",
    "        comment_response = requests.post(\n",
    "            \"http://novel.naver.com/comments/list_comment.nhn\", \n",
    "            headers=headers, \n",
    "            data=data\n",
    "        )\n",
    "        \n",
    "        comment_list += json.loads(comment_response.text.replace(\"\\\\'\", \"\\'\"))[\"comment_list\"]\n",
    "    \n",
    "    df = pd.DataFrame(comment_list)\n",
    "    df.drop(\n",
    "        [\n",
    "            \"comment_no\", \n",
    "            \"enc_writer_id\", \n",
    "            \"enc_writer_profile_user_id\",\n",
    "            \"group_no\",\n",
    "            \"reply_level\",\n",
    "            \"status\",\n",
    "            \"is_mine\",\n",
    "            \"is_reply\",\n",
    "            \"parent_comment_no\",\n",
    "            \"deleted_yn\",\n",
    "            \"is_yozm\",\n",
    "            \"is_me2day\",\n",
    "            \"visible_yn\",\n",
    "            \"object_url\",\n",
    "            \"writer_profile_user_id\"\n",
    "        ], \n",
    "        axis=1, \n",
    "        inplace=True\n",
    "    )\n",
    "    df.to_pickle(os.path.join(\"data\", \"comment\", file_name))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_episode_novel_comment(episode_df, reset=False):\n",
    "    \n",
    "    episode_df = episode_df[episode_df[\"comments_count\"] != 0]\n",
    "    \n",
    "    df_list = list(map(\n",
    "            get_novel_comments,\n",
    "            episode_df[\"ID\"],\n",
    "            episode_df[\"comments_count\"],\n",
    "            [False] * len(episode_df), \n",
    "            episode_df[\"level\"],\n",
    "            [reset] * len(episode_df),\n",
    "            episode_df[\"volume\"],\n",
    "        ))\n",
    "        \n",
    "    episode_comments_df = pd.concat(df_list).reset_index(drop=True)\n",
    "    episode_comments_df.to_csv(os.path.join(\"data\", episode_df[\"level\"][0], \"episode_comments.csv\"))\n",
    "    \n",
    "    return episode_comments_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_main_novel_comment(main_df, reset=False):\n",
    "    \n",
    "    main_df = main_df[main_df[\"comments_count\"] != 0]\n",
    "    \n",
    "    df_list = list(map(\n",
    "            get_novel_comments,\n",
    "            main_df[\"ID\"],\n",
    "            main_df[\"comments_count\"],\n",
    "            [True] * len(main_df),\n",
    "            main_df[\"level\"],\n",
    "            [reset] * len(main_df)\n",
    "        ))\n",
    "        \n",
    "    main_comment_df = pd.concat(df_list).reset_index(drop=True)\n",
    "    main_comment_df.to_csv(os.path.join(\"data\", main_df[\"level\"][0], \"main_comments.csv\"))\n",
    "    \n",
    "    return main_comment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'make_main_novel_comment' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-50a09d83bd24>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mepisode_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_main_novel_comment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"webnovel\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'make_main_novel_comment' is not defined"
     ]
    }
   ],
   "source": [
    "episode_df = make_main_novel_comment(\"webnovel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
